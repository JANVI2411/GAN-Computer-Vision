{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO9cZDI_0K5Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
        "import albumentations as A\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOeJzrGGALnz"
      },
      "source": [
        "# Geneartor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbWxwHrPp7v-"
      },
      "outputs": [],
      "source": [
        "model_architecture=[\n",
        "    (\"C1\",(64,3,1,1)),\n",
        "    (\"D1\",(64,3,2,1)),\n",
        "    (\"C2\",(128,3,1,1)),\n",
        "    (\"D2\",(128,3,2,1)),\n",
        "    (\"C3\",(256,3,1,1)),\n",
        "    (\"D3\",(256,3,2,1)),\n",
        "    (\"C4\",(512,3,1,1)),\n",
        "    (\"D4\",(512,3,2,1)),\n",
        "    (\"B\",(1024,3,1,1)), # bottleneck connection\n",
        "    (\"U1\",(512,3,2,1)),\n",
        "    (\"C5\",(512,3,1,1)), # input_channel= 512+512(from U1 and C4)\n",
        "    (\"U2\",(256,3,2,1)),\n",
        "    (\"C6\",(256,3,1,1)), # input_channel=256+256(from U2 and C3)\n",
        "    (\"U3\",(128,3,2,1)),\n",
        "    (\"C7\",(128,3,1,1)), # input_channel=128+128(from U3 and C2)\n",
        "    (\"U4\",(64,3,2,1)),\n",
        "    (\"C8\",(64,3,1,1)), # input_channel=64+64(from U4 and C1)\n",
        "    (\"C9\",(3,1,1,0)), # output_image\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik-qJV5P0RFQ"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,kernel_size,stride,padding,layer_type=None):\n",
        "    super(CNNBlock,self).__init__()\n",
        "    self.layer_type=layer_type\n",
        "    if not layer_type:\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,padding_mode=\"reflect\"),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(out_channels,out_channels,kernel_size,stride,padding,padding_mode=\"reflect\"),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "    elif layer_type==\"last\":\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,1,1,0),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "    elif layer_type==\"up\":\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels,out_channels,kernel_size,2,padding),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "    elif layer_type==\"down\":\n",
        "      self.layers = nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size,2,padding),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU()\n",
        "      )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,img_channels,model_architecture,feature_g): # latent_dim=(-1,100,1,1),feature_g=64\n",
        "    super(Generator,self).__init__()\n",
        "    self.model_architecture = model_architecture\n",
        "    self.img_channels= img_channels\n",
        "    self.feature_g = feature_g\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.create_network()\n",
        "\n",
        "  def create_network(self):\n",
        "    img_channels = self.img_channels\n",
        "    for layer in self.model_architecture:\n",
        "      if layer[0][0]==\"C\" and int(layer[0][1])<=4:\n",
        "        if int(layer[0][1])==1: # Initial Layer\n",
        "          self.layers.append(CNNBlock(self.img_channels,self.feature_g,3,1,1))\n",
        "          img_channels = self.feature_g\n",
        "        else:\n",
        "          self.layers.append(CNNBlock(img_channels,img_channels*2,3,1,1))\n",
        "          img_channels = img_channels*2\n",
        "      elif layer[0][0]==\"D\":\n",
        "        self.layers.append(CNNBlock(img_channels,img_channels,3,2,1,layer_type=\"down\"))\n",
        "      elif layer[0][0]==\"B\":\n",
        "        self.layers.append(CNNBlock(img_channels,img_channels*2,3,1,1))\n",
        "        img_channels = img_channels*2\n",
        "      elif layer[0][0]==\"U\":\n",
        "        self.layers.append(CNNBlock(img_channels,img_channels//2,4,2,1,layer_type=\"up\"))\n",
        "        img_channels = img_channels//2\n",
        "      elif layer[0][0]==\"C\" and int(layer[0][1])>=5:\n",
        "        if int(layer[0][1])==9: # Last Layer\n",
        "          self.layers.append(CNNBlock(img_channels,self.img_channels,1,1,0,layer_type=\"last\"))\n",
        "        else:\n",
        "          self.layers.append(CNNBlock(img_channels*2,img_channels,3,1,1))\n",
        "\n",
        "\n",
        "  def forward(self,x): # input_image and noise\n",
        "    skip_connections=[]\n",
        "    for idx,layer in enumerate(self.model_architecture):\n",
        "      if layer[0] in [\"C1\",\"C2\",\"C3\",\"C4\"]:\n",
        "        x = self.layers[idx](x)\n",
        "        skip_connections.append(x)\n",
        "      elif layer[0]==\"C9\": # last layer\n",
        "        x = self.layers[idx](x)\n",
        "      elif layer[0] in [\"C5\",\"C6\",\"C7\",\"C8\"]:\n",
        "          x = torch.cat([skip_connections[-1],x],dim=1)\n",
        "          x = self.layers[idx](x)\n",
        "          skip_connections.pop()\n",
        "      else:\n",
        "        x = self.layers[idx](x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ehOHpdYGdt-"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "\n",
        "# class Block(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "#         super(Block, self).__init__()\n",
        "#         self.conv = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "#             if down\n",
        "#             else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "#             nn.BatchNorm2d(out_channels),\n",
        "#             nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
        "#         )\n",
        "\n",
        "#         self.use_dropout = use_dropout\n",
        "#         self.dropout = nn.Dropout(0.5)\n",
        "#         self.down = down\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv(x)\n",
        "#         return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, in_channels=3, features=64):\n",
        "#         super().__init__()\n",
        "#         self.initial_down = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#         )\n",
        "#         self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
        "#         self.down2 = Block(\n",
        "#             features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
        "#         )\n",
        "#         self.down3 = Block(\n",
        "#             features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "#         )\n",
        "#         self.down4 = Block(\n",
        "#             features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "#         )\n",
        "#         self.down5 = Block(\n",
        "#             features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "#         )\n",
        "#         self.down6 = Block(\n",
        "#             features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "#         )\n",
        "#         self.bottleneck = nn.Sequential(\n",
        "#             nn.Conv2d(features * 8, features * 8, 4, 2, 1), nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
        "#         self.up2 = Block(\n",
        "#             features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
        "#         )\n",
        "#         self.up3 = Block(\n",
        "#             features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
        "#         )\n",
        "#         self.up4 = Block(\n",
        "#             features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
        "#         )\n",
        "#         self.up5 = Block(\n",
        "#             features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
        "#         )\n",
        "#         self.up6 = Block(\n",
        "#             features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
        "#         )\n",
        "#         self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
        "#         self.final_up = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(features * 2, in_channels, kernel_size=4, stride=2, padding=1),\n",
        "#             nn.Tanh(),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         d1 = self.initial_down(x)\n",
        "#         d2 = self.down1(d1)\n",
        "#         d3 = self.down2(d2)\n",
        "#         d4 = self.down3(d3)\n",
        "#         d5 = self.down4(d4)\n",
        "#         d6 = self.down5(d5)\n",
        "#         d7 = self.down6(d6)\n",
        "#         bottleneck = self.bottleneck(d7)\n",
        "#         up1 = self.up1(bottleneck)\n",
        "#         up2 = self.up2(torch.cat([up1, d7], 1))\n",
        "#         up3 = self.up3(torch.cat([up2, d6], 1))\n",
        "#         up4 = self.up4(torch.cat([up3, d5], 1))\n",
        "#         up5 = self.up5(torch.cat([up4, d4], 1))\n",
        "#         up6 = self.up6(torch.cat([up5, d3], 1))\n",
        "#         up7 = self.up7(torch.cat([up6, d2], 1))\n",
        "#         return self.final_up(torch.cat([up7, d1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX5NXIjPsF4D",
        "outputId": "aa2b17ef-013b-4d02-977f-3d9dd33e5176"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 128, 128])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Test Generator\n",
        "x = torch.rand((2,3,128,128))\n",
        "gen = Generator(3,model_architecture,64)\n",
        "gen(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySeB4Y4KAHj6"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No49lTaXrwgS"
      },
      "outputs": [],
      "source": [
        "class CNN_Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(CNN_Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(in_channels * 2,features[0],kernel_size=4,stride=2,padding=1,padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(CNN_Block(in_channels, feature, stride=1 if feature == features[-1] else 2))\n",
        "            in_channels = feature\n",
        "\n",
        "        #last layer\n",
        "        layers.append(\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"),\n",
        "        )\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat([x, y], dim=1)\n",
        "        x = self.initial(x)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpIiksyIARNK"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zKxAAJItwan",
        "outputId": "6728998b-30c2-410b-8adb-5d6f4198e592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
            "30168306/30168306 [==============================] - 11s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pathlib\n",
        "\n",
        "dataset_name = \"facades\"\n",
        "\n",
        "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    fname=f\"{dataset_name}.tar.gz\",\n",
        "    origin=_URL,\n",
        "    extract=True)\n",
        "\n",
        "path_to_zip  = pathlib.Path(path_to_zip)\n",
        "\n",
        "PATH = path_to_zip.parent/dataset_name\n",
        "\n",
        "# _URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/maps.tar.gz'\n",
        "# path_to_zip = tf.keras.utils.get_file('maps.tar.gz',\n",
        "#                                       origin=_URL,\n",
        "#                                       extract=True)\n",
        "\n",
        "# PATH = os.path.join(os.path.dirname(path_to_zip), 'maps/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjSpwbR_xjSz",
        "outputId": "af7745cd-e6f3-46f9-e56e-c56021291aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test  train  val\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.keras/datasets/facades/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9VKygJQx7AO",
        "outputId": "33203734-f8cd-46c3-e18a-f3d91931e7af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(256, 512, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array(Image.open(\"/root/.keras/datasets/facades/test/11.jpg\")).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVeYgIOet27z"
      },
      "outputs": [],
      "source": [
        "class MapDataset(Dataset):\n",
        "    def __init__(self, root_dir,resize):\n",
        "        self.root_dir = root_dir\n",
        "        self.list_files = os.listdir(self.root_dir)\n",
        "        self.both_transform = A.Compose(\n",
        "            [A.Resize(width=resize, height=resize),], additional_targets={\"image0\": \"image\"},\n",
        "        )\n",
        "\n",
        "        self.transform_only_input = A.Compose(\n",
        "            [\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.ColorJitter(p=0.2),\n",
        "                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.transform_only_mask = A.Compose(\n",
        "            [\n",
        "                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
        "                ToTensorV2(),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_file = self.list_files[index]\n",
        "        img_path = os.path.join(self.root_dir, img_file)\n",
        "        image = np.array(Image.open(img_path))\n",
        "        input_image = image[:, :256, :]\n",
        "        target_image = image[:, 256:, :]\n",
        "\n",
        "        # augmentations = config.both_transform(image=input_image, image0=target_image)\n",
        "        # input_image = augmentations[\"image\"]\n",
        "        # target_image = augmentations[\"image0\"]\n",
        "\n",
        "        input_image = self.transform_only_input(image=input_image)[\"image\"]\n",
        "        target_image = self.transform_only_mask(image=target_image)[\"image\"]\n",
        "\n",
        "        return input_image, target_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK4Zmv0cCx5Q"
      },
      "source": [
        "# Utils and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slpW-8MsCyDQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TRAIN_DIR = \"/root/.keras/datasets/facades/train\"\n",
        "VAL_DIR = \"/root/.keras/datasets/facades/val\"\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS_IMG = 3\n",
        "L1_LAMBDA = 100\n",
        "LAMBDA_GP = 10\n",
        "NUM_EPOCHS = 50\n",
        "SAVE_IMG_DIR = \"/content/evaluation_1\"\n",
        "if not os.path.exists(SAVE_IMG_DIR):\n",
        "  os.makedirs(SAVE_IMG_DIR)\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
        "CHECKPOINT_GEN = \"gen.pth.tar\"\n",
        "\n",
        "def save_some_examples(gen, val_loader, epoch, folder):\n",
        "    x, y = next(iter(val_loader))\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        y_fake = gen(x)\n",
        "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
        "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
        "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
        "        if epoch == 1:\n",
        "            save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
        "    gen.train()\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js5eNqjDAVYG"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3wkaSRJEtEz"
      },
      "outputs": [],
      "source": [
        "def train_fn(\n",
        "    disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        # Train Discriminator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            y_fake = gen(x)\n",
        "            D_real = disc(x, y)\n",
        "            D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
        "            D_fake = disc(x, y_fake.detach())\n",
        "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
        "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
        "\n",
        "        disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generator\n",
        "        with torch.cuda.amp.autocast():\n",
        "            D_fake = disc(x, y_fake)\n",
        "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
        "            L1 = l1_loss(y_fake, y) * L1_LAMBDA\n",
        "            G_loss = G_fake_loss + L1\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            loop.set_postfix(\n",
        "                D_loss=D_loss.item(),\n",
        "                G_loss = G_fake_loss.item(),\n",
        "                G_L1_loss=L1.item(),\n",
        "            )\n",
        "\n",
        "\n",
        "def main():\n",
        "    disc = Discriminator(in_channels=3).to(DEVICE)\n",
        "    gen = Generator(in_channels=3, features=64).to(DEVICE)\n",
        "    # gen = Generator(img_channels=3,model_architecture=model_architecture,feature_g=64).to(DEVICE)\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999),)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "    BCE = nn.BCEWithLogitsLoss()\n",
        "    L1_LOSS = nn.L1Loss()\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE,\n",
        "        )\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "    train_dataset = MapDataset(TRAIN_DIR,IMAGE_SIZE)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "    )\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "    val_dataset = MapDataset(VAL_DIR,IMAGE_SIZE)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(\n",
        "            disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler,\n",
        "        )\n",
        "\n",
        "        if SAVE_MODEL and epoch % 5 == 0:\n",
        "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n",
        "\n",
        "        save_some_examples(gen, val_loader, epoch, folder=SAVE_IMG_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH18j_8RG1kg",
        "outputId": "960ba075-6f8f-486a-dc4a-605bd08c5709"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:06<00:00,  3.95it/s, D_loss=0.455, G_L1_loss=54.9, G_loss=1.26]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.55it/s, D_loss=0.336, G_L1_loss=42, G_loss=1.88]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.42it/s, D_loss=0.175, G_L1_loss=40.5, G_loss=2.29]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.28it/s, D_loss=0.139, G_L1_loss=42.2, G_loss=2.62]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.51it/s, D_loss=0.452, G_L1_loss=41, G_loss=1.3]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.85it/s, D_loss=0.544, G_L1_loss=41.6, G_loss=2.17]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.44it/s, D_loss=0.179, G_L1_loss=35.8, G_loss=2.02]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.47it/s, D_loss=0.576, G_L1_loss=33.3, G_loss=0.734]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.16it/s, D_loss=0.0927, G_L1_loss=39.2, G_loss=3.03]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.40it/s, D_loss=0.11, G_L1_loss=39.7, G_loss=3.45]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.05it/s, D_loss=0.201, G_L1_loss=38.1, G_loss=2.77]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.14it/s, D_loss=0.0911, G_L1_loss=36.8, G_loss=3.1]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.42it/s, D_loss=0.442, G_L1_loss=32.1, G_loss=2.39]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.13it/s, D_loss=0.292, G_L1_loss=38.2, G_loss=1.61]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.40it/s, D_loss=0.243, G_L1_loss=35.5, G_loss=2.54]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.28it/s, D_loss=0.0914, G_L1_loss=38.4, G_loss=3.26]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.73it/s, D_loss=0.172, G_L1_loss=35.2, G_loss=2.77]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.35it/s, D_loss=0.172, G_L1_loss=37.1, G_loss=1.75]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.24it/s, D_loss=0.266, G_L1_loss=36.2, G_loss=2.52]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.14it/s, D_loss=0.294, G_L1_loss=31.7, G_loss=1.66]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.24it/s, D_loss=0.268, G_L1_loss=33.7, G_loss=1.77]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.72it/s, D_loss=0.15, G_L1_loss=34.5, G_loss=2.55]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.32it/s, D_loss=0.233, G_L1_loss=32.7, G_loss=2.5]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.30it/s, D_loss=0.326, G_L1_loss=30.2, G_loss=1.39]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.12it/s, D_loss=0.286, G_L1_loss=30.9, G_loss=2.72]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.26it/s, D_loss=0.236, G_L1_loss=32.1, G_loss=2.28]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.77it/s, D_loss=0.262, G_L1_loss=28.8, G_loss=1.65]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.29it/s, D_loss=0.326, G_L1_loss=28, G_loss=1.53]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.28it/s, D_loss=0.313, G_L1_loss=28.8, G_loss=2.49]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.94it/s, D_loss=0.531, G_L1_loss=24.6, G_loss=1.44]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.26it/s, D_loss=0.518, G_L1_loss=28.4, G_loss=2.13]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.76it/s, D_loss=0.251, G_L1_loss=28.5, G_loss=2.04]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.29it/s, D_loss=0.237, G_L1_loss=25.8, G_loss=1.42]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.25it/s, D_loss=0.899, G_L1_loss=25.4, G_loss=0.884]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.89it/s, D_loss=0.398, G_L1_loss=26.8, G_loss=2.51]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.25it/s, D_loss=0.282, G_L1_loss=25.3, G_loss=1.87]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.78it/s, D_loss=0.507, G_L1_loss=21.5, G_loss=1.6]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.22it/s, D_loss=0.105, G_L1_loss=27, G_loss=2.92]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.21it/s, D_loss=0.24, G_L1_loss=23.3, G_loss=1.45]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.98it/s, D_loss=0.382, G_L1_loss=23, G_loss=1.48]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.20it/s, D_loss=0.591, G_L1_loss=24.9, G_loss=1.86]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.86it/s, D_loss=1.42, G_L1_loss=24.1, G_loss=1.67]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.07it/s, D_loss=0.58, G_L1_loss=21.6, G_loss=2.06]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.20it/s, D_loss=0.774, G_L1_loss=18.9, G_loss=0.771]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.99it/s, D_loss=0.933, G_L1_loss=22.1, G_loss=1.99]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.22it/s, D_loss=0.577, G_L1_loss=21.4, G_loss=1.26]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.82it/s, D_loss=0.68, G_L1_loss=18.2, G_loss=2.18]\n",
            "100%|██████████| 25/25 [00:05<00:00,  4.98it/s, D_loss=0.589, G_L1_loss=21.8, G_loss=2.02]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.20it/s, D_loss=0.543, G_L1_loss=21.4, G_loss=1.99]\n",
            "100%|██████████| 25/25 [00:04<00:00,  5.02it/s, D_loss=0.464, G_L1_loss=21, G_loss=1.61]\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1bLM7yP1ug7",
        "outputId": "e8101844-79b0-4524-deaa-7bc5ffd14fdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 25/25 [00:15<00:00,  1.62it/s, D_loss=0.39, G_loss=60]\n",
            "100%|██████████| 25/25 [00:15<00:00,  1.61it/s, D_loss=0.282, G_loss=53.6]\n",
            "100%|██████████| 25/25 [00:15<00:00,  1.58it/s, D_loss=0.285, G_loss=48.4]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.55it/s, D_loss=0.167, G_loss=46.6]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.53it/s, D_loss=0.474, G_loss=46.7]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.53it/s, D_loss=0.56, G_loss=42.8]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.51it/s, D_loss=0.352, G_loss=38.4]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.48it/s, D_loss=0.138, G_loss=42.8]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.48it/s, D_loss=0.312, G_loss=37.4]\n",
            "100%|██████████| 25/25 [00:16<00:00,  1.47it/s, D_loss=0.107, G_loss=38.8]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.46it/s, D_loss=0.545, G_loss=33.6]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.45it/s, D_loss=0.412, G_loss=38.3]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.45it/s, D_loss=0.139, G_loss=38.4]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.43it/s, D_loss=0.666, G_loss=40.4]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.41it/s, D_loss=0.308, G_loss=36]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.45it/s, D_loss=0.336, G_loss=37.5]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.41it/s, D_loss=0.227, G_loss=35.7]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.36it/s, D_loss=0.323, G_loss=30.6]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.38it/s, D_loss=0.449, G_loss=33.2]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.41it/s, D_loss=0.974, G_loss=34.5]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.37it/s, D_loss=0.62, G_loss=28.1]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.146, G_loss=35.5]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.365, G_loss=31.2]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.39it/s, D_loss=0.951, G_loss=31.3]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.439, G_loss=31.6]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.291, G_loss=34.7]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.359, G_loss=30.4]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.202, G_loss=31.3]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.656, G_loss=28.6]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.27, G_loss=30.2]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.201, G_loss=31]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.173, G_loss=29.4]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.307, G_loss=30.6]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.38it/s, D_loss=0.526, G_loss=29.7]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.792, G_loss=31.5]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.39it/s, D_loss=0.32, G_loss=29.2]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.624, G_loss=25.9]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.226, G_loss=27]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.705, G_loss=30.4]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.388, G_loss=30.2]\n",
            "100%|██████████| 25/25 [00:18<00:00,  1.38it/s, D_loss=0.645, G_loss=23]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.341, G_loss=25.5]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.725, G_loss=27.2]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.41it/s, D_loss=0.5, G_loss=27.5]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.615, G_loss=26.7]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.763, G_loss=23.1]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.334, G_loss=25.3]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.423, G_loss=28]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.40it/s, D_loss=0.296, G_loss=28.3]\n",
            "100%|██████████| 25/25 [00:17<00:00,  1.39it/s, D_loss=0.382, G_loss=25.4]\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hWYQsRR2IXv"
      },
      "outputs": [],
      "source": [
        "# !rm *.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km6C0nMLA9Mm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}